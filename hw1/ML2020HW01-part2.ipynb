{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 1: Part 2 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a Bayes theorem, we can write down the target probability as:\n",
    "\n",
    "$$\n",
    "p(x = 1 | m \\text{ ones}, (n - m) \\text{ zeros}) = \n",
    "\\frac{p(m \\text{ ones}, (n - m) \\text{ zeros} | x = 1) p(x = 1)}{p(m \\text{ ones}, (n - m) \\text{ zeros})}.\n",
    "$$\n",
    "\n",
    "In the numerator we clearly see the Bernoulli process of having $(n - m)$ assessors making a mistake multiplied by prior probability of $x$ belonging to the class $1$. However, to calculate denominator, we will need to use the full probability formula.\n",
    "\n",
    "$$\n",
    "p(x = 1 | m \\text{ ones}, (n - m) \\text{ zeros}) =  \\\\\n",
    "\\frac{C_n^{n - m} (1 - q)^m q^{n - m} p}{p(m \\text{ ones},(n - m) \\text{ zeros}| x = 0) p(x = 0) + p(m \\text{ ones}, (n - m) \\text{ zeros} | x = 1) p(x = 1)} = \\\\\n",
    "\\frac{C_n^{n - m} (1 - q)^m q^{n - m} p}{C_n^m q^m (1 - q)^{n - m} (1 - p) + C_n^{n - m} (1 - q)^m q^{n - m} p} = \n",
    "\\frac{(1 - q)^m q^{n - m} p}{q^m (1 - q)^{n - m} (1 - p) + (1 - q)^m q^{n - m} p}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Linear Regression (1 point)\n",
    "Let us consider the problem of linear regression for 2D data $(x_{1},y_{1}),\\dots,(x_{n},y_{n})\\in\\mathbb{R}^{2+ 1}$. Let us have $l_{\\infty}$ regularization penalty, i.e. the optimization problem is\n",
    "$$\n",
    "||Xw - y||_2^2 + \\lambda||w||_{\\infty} \\rightarrow \\min_{\\boldsymbol{w}}\n",
    "$$\n",
    "\n",
    "Show that this problem is equal to Lasso regression problem with feature matrix $Z = XA \\in \\mathbb{R}^{n \\times 2}$ for a certain $2 \\times 2$ matrix $A$ and the same target $y$.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Probit Regression (1 point)\n",
    "Let us consider the Probit regression model for a binary classification problem. It is a linear model analogous to the Logistic Regression. For a feature vector $x \\in \\mathbb{R}^d$ the probability for label $y$ being equal to 1 is given by\n",
    "$$P(y=1|x) = \\Phi(x^Tw + b).$$ \n",
    "Here $\\Phi(x)=\\int_{-\\infty}^{x}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^{2}}{2}}dt$ is the Cumulative Density Function for the **standard normal distribution**; values $w \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$ are learnable parameters of the probit regression model.\n",
    "\n",
    "Write down the optimization problem for training probit  probit regression **without L2-regularization** and show that it does not have optimum in the case when the training set is **lineary separable**.\n",
    "\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Multiclass Bayesian Naive Classifier (1 point)\n",
    "Let us consider multiclass classification problem with classes $C_1, \\ldots, C_K$. Assume that all $d$ features $x_1, \\dots,x_d$ are binary, i.e. $x_{i}\\in\\{0,1\\}$ for $i=1,2\\dots,d$. Show that the decision rule of a Bayesian Naive Classifier can be represented as $\\arg \\max$ of linear functions of the input. \n",
    "\n",
    "### Your solution:\n",
    "Conditional distribution for bayesian classifier with binary features is:\n",
    "$$\n",
    "p(X|C_k) = \\prod_{j=1}^{d}p_{k_i}^{x_i}(1 − p_{ki})^{(1−x_i)}.\n",
    "$$\n",
    "\n",
    "For naive classification we are interested in finding the mode of posterior labels distribution. Since denominator in Bayes theorem is a constant, function for optimization is: \n",
    "$$\n",
    "L = \\log p(C_k) + \\sum_{j=1}^{d} x_i \\log~p_{k_i} + (1-x_i) \\log (1-p_{k_i}).\n",
    "$$\n",
    "\n",
    "In other words, mode $\\hat{y}$ can be found as:\n",
    "$$\n",
    "\\hat{y} = \n",
    "argmax \\left[ \\log p(C_k) + \\sum_{j=1}^{d} x_i \\log~p_{k_i} + (1-x_i) \\log (1-p_{k_i}) \\right] = \n",
    "argmax \\left[ \\sum_{j=1}^{d} x_i \\log p_{k_i} + (1-x_i) \\log (1-p_{k_i}) \\right] = \\\\\n",
    "argmax \\left[ \\sum_{j=1}^{d} x_i \\log p_{k_i} + -x_i \\log (1-p_{k_i}) + \\log (1-p_{k_i}) \\right] = \n",
    "argmax \\left[ \\sum_{j=1}^{d} x_i \\log \\frac{p_{k_i}}{1-p_{k_i}} + \\log (1-p_{k_i}) \\right].\n",
    "$$ \n",
    "\n",
    "So we've got desired linear model, \n",
    "$$\n",
    "\\hat{y} = argmax[w^Tx + b], \\text{where } b = \\log p(C_k) + \\sum_{j=1}^{d} \\log (1-p_{k_i}); w_i = \\log \\frac{p_{k_i}}{1 - p_{k_i}}.\n",
    "$$\n",
    "[:|||:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Nearest Neighbors (1 point)\n",
    "Consider the 1-nearest-neighbor classifier applied to multiclass classification problem. Let's denote the classifier fitted on data $X$ by $f_X(\\cdot)$.\n",
    "\n",
    "The formula for complete **leave-k-out cross-validation** on data sample $X^{n}$ is defined as\n",
    "$$L_{k}OCV=\\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k}\\frac{1}{k}\\bigg(\\sum_{i\\in X}[y_{i}\\neq f_{X^{n}\\setminus X}( x_{i})]\\bigg),$$\n",
    "where $\\mathcal{P}(X^{n})$ is the set of all subsets of $X^{n}$. For all $i=1,2\\dots,n$ denote the label of $m$-th closest neighbor of $x_{i}$ in $X^{n}\\setminus \\{x_{i}\\}$ by $y_{i}^{m}$. Show that \n",
    "$$L_{k}OCV=\\sum_{m=1}^{k}\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq y_{i}^{m}]}_{K_{m}(X^{n})}\\frac{C_{n-1-m}^{n-k-1}}{C_{n-1}^{k-1}},$$\n",
    "where $K_{m}(X^{n})$ is called the compactness profile of $X^{n}$, i.e. the fraction of objects whose $m$-th nearest neighbor has different label. For convenience assume that all the pairwise distances between objects are different.\n",
    "### Your solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Bootstrap (1 point)\n",
    "Let the subsample $\\hat{X}^{n}$ of size $n$ be generated from sample $X^{n}=\\{\\boldsymbol{x}_{1},\\dots\\boldsymbol{x}_{n}\\}$ by bootstrap procedure. Find the probability that object $x_{i}$ is not present in $\\hat{X}^{n}$ and compute the limit of this probability for $n\\rightarrow\\infty$.\n",
    "### Your solution:\n",
    "The solution is straightforward using famous limit equation for $e$.\n",
    "The probability of object $x_{i} \\in \\hat{X}^{n}$: $P(x_{i} \\in \\hat{X}^{n}) = \\left (1- \\frac{1}{n} \\right)^n$.\n",
    "\n",
    "Thus, the answer is: $\\lim \\limits_{n \\rightarrow \\infty} P(x_{i} \\in \\hat{X}^{n}) = e^{-1} \\approx 0.367$. [:|||:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6. Decision Tree Leaves (1+1=2 points)\n",
    "\n",
    "Consider a leaf of a binary decision tree which consists of object-label pairs $(x_{1},y_{1}),\\dots,(x_{n},y_{n})$. The prediction $\\hat{y}$ of this leaf is defined to minimize the loss on the training sample.\n",
    "\n",
    "* For a classification tree for K classes ($y_{i}\\in\\{1,2,\\dots,K\\}$) and zero-one loss $L(y,\\hat{y})=[y\\neq \\hat{y}]$, find the optimal prediction in the leaf.\n",
    "\n",
    "* For a regression tree ($y_{i}\\in\\mathbb{R}$) and squared percentage error loss $L(y,\\hat{y})=\\frac{(y-\\hat{y})^{2}}{y^2}$ find the optimal prediction in the leaf.\n",
    "\n",
    "\n",
    "\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7. Classification (1 point)\n",
    "Let objects $\\boldsymbol{x}_{1},\\dots,\\boldsymbol{x}_{n}$ have binary labels $y_{1}, y_{2},\\dots,y_{n}\\in\\{0,1\\}$. Let the classifier $f$ assign to objects probabilities of being from class $1$. Without loss of generality assume that $f(\\boldsymbol{x_{i}})<f(\\boldsymbol{x_{j}})$ for all $i<j$. Denote the number of objects of $X^{n}$ from class $1$ by $n_{1}=\\sum_{i=1}^{n}[y_{i}=1]$. Show that \n",
    "$$S_{\\text{ROC}}(f,X^{n})=\\frac{1}{n_{1}(n-n_{1})}\\sum_{i<j}[y_{i}<y_{j}]$$\n",
    "where $S_{\\text{ROC}}(f,X^{n})$ is the Area Under ROC of classifier $f$.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8. Kernels (1+1=2 points)\n",
    "Kernel $K(x,y)$ corresponds to dot product of feature maps $\\varphi$ and therefore $K(x,y) = \\langle \\varphi(x), \\varphi(y) \\rangle$. Derive functions $\\varphi$ for the following kernels:\n",
    "* $K(x,y)=\\langle x, y \\rangle ^ d$;\n",
    "* $K(x,y)= \\left(c + \\langle x, y \\rangle \\right)^ d$  with $c\\geq 0$;\n",
    "\n",
    "### Your solution:\n",
    "#### Task 8.1\n",
    "$$<x,y>^d = \n",
    "(\\sum \\limits_{i=1}^n x_i y_i)^d = \\sum \\limits_{|\\alpha| = d} C_{d}^{\\alpha_1} C_{d - \\alpha_1}^{\\alpha_2}...C_{\\alpha_n}^{\\alpha_n} (x_1y_1)^{\\alpha_1}...(x_ny_n)^{\\alpha_n} = \n",
    "\\sum \\limits_{|\\alpha| = d} \\frac{d! (d - \\alpha_1)! ... \\alpha_n!}{\\alpha_1! (d - \\alpha_1)! ... \\alpha_n!} (x_1y_1)^{\\alpha_1}...(x_ny_n)^{\\alpha_n} = \\\\\n",
    "= \\sum \\limits_{|\\alpha| = d} \\sqrt{\\frac{d! (d - \\alpha_1)! ... \\alpha_n!}{\\alpha_1! (d - \\alpha_1)! ... \\alpha_n!}} (x_1)^{\\alpha_1}...(x_n)^{\\alpha_n} \\sqrt{\\frac{d! (d - \\alpha_1)! ... \\alpha_n!}{\\alpha_1! (d - \\alpha_1)! ... \\alpha_n!}} (y_1)^{\\alpha_1}...(y_n)^{\\alpha_n} = <\\phi(x),\\phi(y)>.\n",
    "$$\n",
    "\n",
    "So, the function we're looking for is:\n",
    "$\\phi(x) = (x_1^d,...,\\sqrt{\\frac{d! (d - alpha_1)! ... \\alpha_n!}{\\alpha_1! (d - \\alpha_1)! ... \\alpha_n!}} (x_1)^{\\alpha_1}...(x_n)^{\\alpha_n},...,x_n^d)$, where $\\alpha\\in \\mathbb{N}^n, |\\alpha| = d$.\n",
    "\n",
    "#### Task 8.2\n",
    "$$\n",
    "(c + <x,y>)^d = \n",
    "(c + \\sum \\limits_{i=1}^n x_iy_i)^d = \n",
    "\\sum \\limits_{|\\alpha| = d} C_{d}^{\\alpha_1}C_{d-\\alpha_1}^{\\alpha_2}...C_{\\alpha_n}^{\\alpha_n}(x_1y_1)^{\\alpha_1}...(x_ny_n)^{\\alpha_n} = \n",
    "\\sum \\limits_{|\\alpha| = d} \\frac{d! (d - \\alpha_1)! ... \\alpha_n!}{\\alpha_1! (d - \\alpha_1)! ... \\alpha_n!} (x_1y_1)^{\\alpha_1}...(x_ny_n)^{\\alpha_n} = \\\\\n",
    "= \\sum \\limits_{|\\alpha| = d} \\sqrt{\\frac{d! (d - \\alpha_1)! ... \\alpha_n!}{\\alpha_1! (d - \\alpha_1)! ... \\alpha_n!}} (x_1)^{\\alpha_1}...(x_n)^{\\alpha_n} \\sqrt{\\frac{d! (d - \\alpha_1)! ... \\alpha_n!}{\\alpha_1! (d - \\alpha_1)! ... \\alpha_n!}} (y_1)^{\\alpha_1}...(y_n)^{\\alpha_n} = <\\phi(x),\\phi(y)>.\n",
    "$$\n",
    "\n",
    "So, function we're looking for is:\n",
    "$\\phi(x) = (x_1^d,...,\\sqrt{\\frac{d! (d - \\alpha_1)! ... \\alpha_n!}{\\alpha_1! (d - \\alpha_1)! ... \\alpha_n!}} (x_1)^{\\alpha_1}...(x_n)^{\\alpha_n},...,x_n^d)$, where $\\alpha\\in \\mathbb{N}^n, |\\alpha| = d$. \n",
    "[:|||:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9. Kernel Methods (1 point)\n",
    "Prove that Gaussian Kernel $K(x,x')=\\exp(-\\|x-x'\\|^{2})$ is positive definite. You do not need to prove that the linear kernel is positive definite.\n",
    "### Your solution:\n",
    "\n",
    "From Mercer theorem we know that kernel must be symmetric and positive definite:\n",
    "\n",
    " 1. $K(x,x') = K(x',x)$ is obviously symmetric\n",
    " 2. Let $(x_1, ..., x_k)$ be a data, $x_i \\in \\mathbb{R}$, then $K = (K(x_i, x_j))_{i, j = 1}^{d}$ is a matrix for the kernel.\n",
    " Then we need to prove that for $c_1, ..., c_d \\in \\mathbb{R}$ the inequality $\\sum \\limits_{i = 1}^{d} \\sum \\limits_{i = 1}^{d} c_i c_j K(x_i, x_j) \\geq 0$.\n",
    " \n",
    "If we write the kernel and compute square in exponent, we get $K(x_i, x_j) = \\exp(-||x_i||^2) \\exp(-||x_j||^2) \\exp(-2 <x_i, x_j>)$. If now substitute it in the inequality above, we can easily get lower bound:\n",
    "\n",
    "$$\n",
    "\\sum \\limits_{i = 1}^{d} \\sum \\limits_{i = 1}^{d} c_i c_j \\exp(-||x_i||^2) \\exp(-||x_j||^2) \\exp(-2 <x_i, x_j>) \\geq\n",
    "\\exp (- 2 \\max \\limits_{j} x_j) \\sum \\limits_{i = 1}^{d} \\sum \\limits_{i = 1}^{d} c_i c_j \\exp(-2 <x_i, x_j>) \\geq 0.\n",
    "$$\n",
    "\n",
    "The last expression consists of exponent, which is positive, and of the kernel, which is positive definite. [:|||:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10. Support Vector Machine (1 point)\n",
    "Show that for two-class SVM classifier the following upper bound on accuracy leave-one-out-cross-validation estimate holds true:\n",
    "$$L_{1}OCV=\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}=f_{i}(x_{i})]\\leq \\frac{|SV|}{n},$$\n",
    "where for all $i=1,\\dots,n$ $f_{i}(x_{i})$ is SVM fitted on the entire data without observation $(x_{i},y_{i})$ and $|SV|$ is the number of support vectors of SVM fit on the entire data.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
