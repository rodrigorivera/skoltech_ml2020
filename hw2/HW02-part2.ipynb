{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 2: Part 2 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1. Bayesian methods (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a dataset $D =(X,y) =\\{(x_i,y_i)\\}^m_{i=1}$, $x_i \\in \\mathbb{R}^d$, $y_i\\in\\mathbb{R}$ it is known,that \n",
    "$$y_i = w^T x_i + \\epsilon$$\n",
    "where $\\epsilon \\sim N(0,\\sigma^2)$, $w  \\sim N(0,\\alpha I)$ . Suppose that $X^T X =I$, where $I$ is the identity matrix. Derive MAP estimation for $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution\n",
    "From Bayes theorem we know that $p(w | X, y) = \\frac{p(y | X, w) p(w)}{p(y | X)}$.\n",
    "\n",
    "Let's take the logarithm of both parts and write likelyhood of the data and prior distribution of weight:\n",
    "$$\n",
    "\\log p(w | X, y) = \n",
    "\\log \\frac{p(y| X, w) p(w)}{p(y | X)} = \n",
    "\\log p(y |X, w) + \\log p(w) - \\log p(y | X) = \\\\\n",
    "\\log \\prod \\limits_{i=1}^{m} \\mathcal{N} \\left(y_i | w^T x_i, \\sigma^2 \\right) - \\log \\alpha^{m / 2} - \\log  2 \\pi^{m / 2} - \\frac{1}{2 \\alpha} w^T w + \\log p(y | X) = \\\\\n",
    "m\\log \\frac{1}{\\sqrt{2\\pi\\sigma^2}} - \\frac{1}{2 \\sigma^2} \\sum\\limits_{i=1}^{m} \\left(y_i - w^Tx_i\\right)^2 - \\log \\alpha^{m / 2} - \\log  (2 \\pi)^{m / 2} - \\frac{1}{2 \\alpha} w^T w + \\log p(y | X) = \\\\\n",
    "const - \\frac{1}{2}\\left(\\frac{w^T w}{\\alpha} + \\frac{1}{\\sigma^2} \\sum\\limits_{i=1}^{m}\\left(y_i^2 - 2w^T x_i y_i + w^T x_i w^T x_i\\right) \\right).\n",
    "$$\n",
    "\n",
    "With this let's take a derivative to find MAP estimation.\n",
    "\n",
    "$$\n",
    "MAP(w) = \n",
    "argmin_{w} \\left[\\left(\\frac{w^T w}{\\alpha} + \\frac{1}{\\sigma^2} \\sum \\limits_{i=1}^{m}\\left(y_i^2 - 2w^T x_i y_i + w^T w \\right) \\right) \\right].\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\left(\\frac{w^T w}{\\alpha} + \\frac{1}{\\sigma^2} \\sum \\limits_{i=1}^{m}\\left(y_i^2 - 2w^T x_i y_i + w^T w \\right) \\right)}{\\partial w} =\n",
    "(\\frac{1}{\\sigma} + \\frac{1}{\\alpha}) w - \\frac{1}{\\sigma^2} \\sum \\limits_{i=1}^{m} x_i y_i = 0 \\\\\n",
    "\\implies w = \\frac{\\sum \\limits_{i=1}^{m} x_i y_i}{(\\sigma + \\frac{\\sigma^2}{\\alpha})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Gaussian Processes 1 (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\sigma_n(\\mathbf{x}_*)$ be a predictive variance at point $\\mathbf{x}_*$ of a Gaussian Process $f_n$ with zero mean and covariance $k(\\cdot,\\cdot)$ that was built using first $n$ training points.\n",
    "Prove that for $\\forall \\mathbf{x}_*$ it holds\n",
    "$$\n",
    "    \\sigma_{n}(\\mathbf{x}_*) \\leq \\sigma_{n-1}(\\mathbf{x}_*).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3. Gaussian Processes 2 (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider you have gaussian distribution on $R$ with zero mean and differentiable by arguments covariation funtion $k(x, \\tilde{x})$.Get an expression for the correlation between the implementation of a Gaussian process  $y(x) âˆ¼ GP (0, k(x, x ^{\\prime}))$ and its derivative $\\frac{\\partial y(\\tilde x)}{\\partial \\tilde x}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution\n",
    "Let's denote $z(x) = \\frac{\\partial y}{\\partial x}$.\n",
    "Let's recall that:\n",
    "$$\n",
    "corr(x, y) = \\frac{cov(x, y)}{\\sqrt{\\sigma(x) \\sigma(y))}}.\n",
    "$$\n",
    "\n",
    "For our problem, let's firstly compute numerator:\n",
    "$$\n",
    "cov(y(x), z(\\tilde{x})) = \n",
    "\\mathbb{E} y(x) z(\\tilde{x}) - \\mathbb{E} y(x) \\mathbb{E} z(\\tilde{x}) = \n",
    "\\mathbb{E} y(x) \\frac{\\partial y(\\tilde{x})}{\\partial \\tilde{x}} = \\\\\n",
    "\\frac{\\partial \\mathbb{E} y(x) y(\\tilde{x})}{\\partial \\tilde{x}} = \n",
    "\\frac{\\partial K(x, \\tilde{x})}{\\partial \\tilde{x}}.\n",
    "$$\n",
    "\n",
    "Now let's compute denominator \n",
    "\n",
    "1. $$\n",
    "\\sigma (y(x)) = \\mathbb{E} y^2(x) + \\left[ \\mathbb{E} y(x) \\right]^2 = \\mathbb{E} y(x) y(x) = K(x, x).\n",
    "$$\n",
    "2. $$\n",
    "z(x) = \n",
    "\\mathbb{E} z^2(x) + \\left[ \\mathbb{E} z(x) \\right]^2 = \n",
    "\\mathbb{E} \\left( \\frac{\\partial y(\\tilde(x))}{\\partial \\tilde{x}} \\right)^2 = \n",
    "\\mathbb{E} \\left( \\frac{\\partial }{\\partial \\tilde(x)} y(\\tilde{x}) \\frac{\\partial }{\\partial \\tilde(x)} y(\\tilde{x}) \\right) =\n",
    "\\frac{\\partial^2 }{\\partial \\tilde{x}^2} \\mathbb{E} y(\\tilde{x}) y(\\tilde{x}) = \n",
    "\\frac{\\partial^2 }{\\partial \\tilde{x}^2} K(\\tilde{x}, \\tilde{x}).\n",
    "$$\n",
    "\n",
    "Computations above are reasonable because of the two factors. Firstly, differentiation is a linear opearator as well as expextation. Secondly, $y$ is centered.\n",
    "\n",
    "So, finally, we've got the answer:\n",
    "$$\n",
    "corr(y(x), z(\\tilde{x})) = \n",
    "\\frac{\\frac{\\partial K(x, \\tilde{x})}{\\partial \\tilde{x}}}{ \\sqrt{K(x, x) \\frac{\\partial^2 }{\\partial \\tilde{x}^2} K(\\tilde{x}, \\tilde{x})}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4. Kernel theory (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $K(x, x'):\\mathcal{X}\\times \\mathcal{X}\\rightarrow \\mathbb{R}$ be a PDS kernel,\n",
    "and $\\phi\\colon \\mathcal{X} \\to \\mathcal{H}$ its <b>unknown </b> feature mapping. For $x,x'\\in\\mathcal{X}$ derive the formula for the **distance** between $\n",
    "\\phi(x)$ and $\\phi(x')$ in $\\mathcal{H}$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution\n",
    "Given a kernel definition: $K(x, x') = \\langle \\phi(x), \\phi(x') \\rangle$, let's derive distance:\n",
    "$$\n",
    "dist(\\phi(x), \\phi(x')) = \n",
    "\\sqrt{\\langle \\phi(x) - \\phi(x'), \\phi(x) - \\phi(x') \\rangle} =\n",
    "\\sqrt{\\langle \\phi(x), \\phi(x) \\rangle - 2 \\langle \\phi(x), \\phi(x') \\rangle + \\langle \\phi(x'), \\phi(x') \\rangle} = \\\\\n",
    "\\sqrt{K(x, x) - 2 K(x, x') + K(x', x')}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5. Naive Gradient Boosting Regression (1 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given a regression dataset, consisting of 5 samples with 1-dimensional feature vector $X$ and scalar target vector $y \\in \\mathbb{R}$:\n",
    "\n",
    "|  x   |  y   | \n",
    "|:----:|:----:| \n",
    "|  10  |  1   | \n",
    "|  32  |  9   | \n",
    "|  46  |  13  | \n",
    "|  54  |  16  | \n",
    "|  63  |  23  | \n",
    "\n",
    "In this task you are asked to implement **3 steps of Gradient Boosting Regression** with decision tree stumps as the learners $h_0, h_1, h_2$. \n",
    "\n",
    "In order to complete this task:\n",
    "1. Refer to the slides on naive boosting for regression in **Lecture 8**.\n",
    "2. Assume that the initial model $f_0$ is the mean of the target vector $y$\n",
    "3. According to the algorithm on the boosting approach for regression from **1.**, compute the residuals\n",
    "4. Manually, find a suitable split among the $x_i$ for each decision tree weak model $h_t(X)$, which minimizes the loss function:\n",
    "\n",
    "$$L_{\\text{split_i}} = \\frac{\\text{Var}_{left\\_split}*N_{1} + \\text{Var}_{right\\_split}*N_{2}}{N_{1}+N_{2}}$$\n",
    "\n",
    "where  $\\text{Var}$ is the variance of the values contained in each leaf, $N_1$ is the number of target values $y$ in the left leaf, $N_{2}$ - in the right leaf\n",
    "\n",
    "5. Perform the Gradient Boosting step on the ensemble model $f_t$ with the resulting decision tree stump predictions (assume that the learning rate $lr=1.0$).\n",
    "\n",
    "**Note on Decision Tree Stumps:** A decision tree stump is a decision tree, which consists only of the root and its immediate leaves. In case of this task, at each iteration you are asked to consider 5 different variants of the decision tree stumps $h_t^i$ - one variant for each of the split candidates $x_i$. You should choose the variant that minimizes the loss written above. The two leaves of the tree are formed according to the rule:\n",
    "\n",
    "```python\n",
    "if x_i < split:\n",
    "    target_value -> left leaf\n",
    "elif x_i >= split:\n",
    "    target_value -> right leaf\n",
    "```\n",
    "**HINT:** Think about what should be `target_value` equal to in case of Gradient Boosting Regression.\n",
    "\n",
    "The prediction of decision tree stump $h_t(x_i)$ is the mean of the values of the according leaf.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task:\n",
    "\n",
    "* Fill in the table - round the values of table up to the second digit after decimal point:\n",
    "\n",
    "|   x  |   y  |$f_0$|$$y - f_0$$| $L$ |$h_0$|$f_1$|$$y-f_1$$| $L$ |$h_1$|$f_2$|$$y - f_2$$| $L$ |$h_2$|$F_3$|\n",
    "|------|------|-----|-----------|-----|-----|-----|---------|-----|-----|-----|-----------|-----|-----|-----|\n",
    "|  10  |  1   | 12.4|      -11.4|53.44| -7.4|    5|       -4|16.93|-1.42| 3.58|      -2.58| 8.90|-2.58|    1| \n",
    "|  32  |  9   | 12.4|       -3.4|20.95| -7.4|    5|        4|12.93|-1.42| 3.58|       5.42| 7.24| 0.65| 4.23|\n",
    "|  46  |  13  | 12.4|        0.6|16.93| 4.93|17.33|    -4.33|16.93|-1.42|15.92|      -2.92| 7.57| 0.65|16.56|\n",
    "|  54  |  16  | 12.4|        3.6|19.83| 4.93|17.33|    -1.33|13.80|-1.42|15.92|       0.08| 8.90| 0.65|16.56|\n",
    "|  63  |  23  | 12.4|       10.6|25.35| 4.93|17.33|     5.67| 8.91| 5.67|   23|          0| 8.91| 0.65|23.65|\n",
    "\n",
    "\n",
    "where $L$ is the loss, calculated by the formula for decision tree stumps above, for each of the 5 split variants of the decision tree stump at each iteration\n",
    "* Write down the splits (the feature values) you have found for each of the tree stumps\n",
    "\n",
    "* Insert the predictions of the full ensemble model and the split values, you have achieved after 3 iterations into the plotting cell below (**COPY AND PASTE** the last column from the table above and the splits list to the plotting cell below, instead of **#your solution**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean \n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tree(x,F,stumps):\n",
    "    x_range = np.arange(np.min(x), np.max(x)+1)\n",
    "    x_r = []\n",
    "    f_r = []\n",
    "    stmps = [0] + stumps + [np.inf]\n",
    "    for st in range(1,len(stmps)):\n",
    "        x_r.extend([list(group) for k, group in groupby(x_range, lambda x: x<stmps[st] and x>=stmps[st-1]) if k])\n",
    "        f_r.append([f_i for f_i,x_ii in zip(F,x) if x_ii<stmps[st] and x_ii>=stmps[st-1]])\n",
    "    F_to_plot = []\n",
    "    for ft in range(len(f_r)):\n",
    "        #assert len(f_r) == len(x_r)\n",
    "        if len(f_r[ft]) == 1:\n",
    "            F_to_plot.extend([f_r[ft][0]]*len(x_r[ft]))\n",
    "        elif len(f_r[ft]) > 1:\n",
    "            F_to_plot.extend([mean(f_r[ft])]*len(x_r[ft]))\n",
    "    return F_to_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTTING CELL##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [10,32,46,54,63]\n",
    "y = [1, 9, 13, 16, 23]\n",
    "\n",
    "#note that the order of F(x_i) should be corresponding to the order of x_i in the table\n",
    "\n",
    "############ INSERT YOUR SOLUTION HERE###############\n",
    "F3 = np.array([1, 4.53, 16.56, 16.56, 23.65]) ####your solution#####\n",
    "splits = np.array([46, 63, 32]) ####your solution####\n",
    "\n",
    "boosted_F_plot = plot_tree(x, F3, stumps = list(np.sort(splits)))\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "ax.scatter(x,y, label = 'original')\n",
    "ax.scatter(x, F3, label = 'predicted')\n",
    "x_range = np.arange(np.min(x), np.max(x)+1)\n",
    "ax.plot(x_range, boosted_F_plot,'--', linewidth=2, label = 'composite function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6. AdaBoost (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following cases,explain how AdaBoost, as given in **Lecture 7**, will treat a weak hypothesis $h_t$ with weighted error $N_t(h_t , w_t )$. Also, in each case, explain why this behavior takes place.\n",
    "1. $N_t = \\frac{1}{2}$\n",
    "2. $N_t > \\frac{1}{2}$\n",
    "3. $N_t = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution\n",
    "We will need an expression for coefficient alpha for the new weak learner from lectures:\n",
    "$$\n",
    "\\alpha_t = \\frac{1}{2} \\log \\frac{1 - N_t}{N_t}.\n",
    "$$\n",
    "\n",
    "Given the formula above, let's what we've got in each case.\n",
    "1. $N_t = \\frac{1}{2} \\implies \\alpha_t = 0$. This means that we do not take into account new learner.\n",
    "2. $N_t > \\frac{1}{2} \\implies \\alpha_t < 0$. This means that we've got coefficient below zero, so basically, when we multiply it on the predictions of a new classifier, we just invert it's predictions.\n",
    "3. $N_t > \\frac{1}{2} \\implies \\alpha_t = \\infty$. This means that predictions of the previous classifiers are not important at all and we will take into account only the new one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
